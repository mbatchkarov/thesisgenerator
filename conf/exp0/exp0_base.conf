name=exp0
# if true, a bunch of debugging files will be produced
debug=False

training_data=sample-data/reuters21578/r8train-tagged-grouped-small
test_data=sample-data/reuters21578/r8test-tagged-grouped-small
output_dir=./conf/exp0/output

joblib_caching=True

[feature_extraction]
    run=True
    # CountVectorizer, TfIdfVectorizer
#     vectorizer=sklearn.feature_extraction.text.TfidfVectorizer
    vectorizer=thesisgenerator.plugins.bov.ThesaurusVectorizer
    #input_generator=utils.GorkanaXmlParser

    # All arguments that are passed to the Vectorizer class are converted to their
    # python type using ast.literal_eval. This requires the arguments whose correct
    # type is str to be enclosed in quotation marks. Arguments that are left empty
    # will use the default values set in the class constructor.
    input='content'
    charset=
    charset_error=replace
    strip_accents=

    stop_words=english
    token_pattern=
    ngram_range=
    min_n=
    max_n=
    min_df=1
    max_df=
    analyzer=ngram
    preprocessor=
    tokenizer=
    max_features=
    vocabulary=
    binary=
    dtype=

    constrain_vocabulary=True

    #extra options required by the Bag-of-Vectors project Vectorizer
    #comma-separated list; if only a single item is present you MUST add a comma at the end. Whitespace is ignored
    train_thesaurus_files=exp6-12d.strings,
   	#decode_thesaurus=thesisgenerator.utils._vocab_neighbour_source # must be a callable
    sim_threshold=0.15
    k=1
    lemmatize=True
    use_pos=True
    coarse_pos=True
    replace_all=False
    use_tfidf=False
	use_signifier_only=False
	record_stats=False
	sim_compressor=thesisgenerator.utils.linear_compress

	train_token_handler=thesisgenerator.plugins.bov_feature_handlers.BaseFeatureHandler
	decode_token_handler=thesisgenerator.plugins.bov_feature_handlers.BaseFeatureHandler


[tokenizer]
	lowercase=True
	keep_only_IT=True
	remove_stopwords=True
	remove_short_words=True

[crossvalidation]
    #if false, a single run with an 80/20 split is performed
    run=True
    # permitted values are 'kfold', 'skfold', 'loo', 'bootstrap' or 'oracle'. k-fold requires only the 'k' option (number of folds)
    # to be set. 'skfold' performs stratified k-fold. 'bootstrap' required both 'k' (number of bootstraps) and 'ratio'
    # (the proportion of the dataset to include in the train split, 0<ratio<1, the rest of the data is used for testing).
    # If 'oracle' the training data is used for testing too. subsampled_test_set requires k and sample_size
    type=subsampled_test_set
    k=1
    ratio=0.8
    sample_size=20

	stream_data=true
	# seen_data_evaluator needs to be a callable that takes a list of x and y values
	validation_slices=#utils.gorkana_200_seen_positives_validation

[split_data]
    run=true
    stream_data=true

[feature_selection]
    run=False
    # the selection method should be one of the classes defined in
    # sklearn.feature_selection - any parameters the class initialiser takes
    # can be defined below and they will be dynamically assigned to the init call
    method=sklearn.feature_selection.SelectKBest
    scoring_function=sklearn.feature_selection.chi2
    k=1000

[dimensionality_reduction]
    run=False
    #alternatives: sklearn.decomposition.PCA/ ProjectedGradientNMF
    # PCA produces negative feature values (counts) which makes NaiveBayes rather upset
    method=sklearn.decomposition.ProjectedGradientNMF
    #must be less than each of the dimensions of the feature vector matrix
    n_components=5
    whiten=True #only applicable to PCA

[classifiers]
    [[sklearn.naive_bayes.MultinomialNB]]
		run=True

	[[sklearn.naive_bayes.BernoulliNB]]
		run=True

	[[thesisgenerator.classifiers.MultinomialNBWithBinaryFeatures]]
	    run=True

    [[sklearn.neighbors.KNeighborsClassifier]]
		run=False
    	k=10


    [[sklearn.linear_model.LogisticRegression]]
		run=False

    [[sklearn.svm.LinearSVC]]
    	run=True

    [[thesisgenerator.classifiers.MostCommonLabelClassifier]]
	    run=False

[evaluation]

	# the default setting for the sklearn's metrics is to work per-class
    [[sklearn.metrics.precision_score]]
        run=True

	[[sklearn.metrics.recall_score]]
		run=True

	[[sklearn.metrics.f1_score]]
		run=True

    # these metrics wrap sklearn's own to provide micro/macro averaging on
    # the the per-class values provided by the metrics above
    [[thesisgenerator.metrics.macroavg_prec]]
    	run=True

	[[thesisgenerator.metrics.macroavg_rec]]
    	run=True

	[[thesisgenerator.metrics.macroavg_f1]]
    	run=True

    [[thesisgenerator.metrics.microavg_prec]]
    	run=True

	[[thesisgenerator.metrics.microavg_rec]]
    	run=True

	[[thesisgenerator.metrics.microavg_f1]]
    	run=True



# roc_curve runs weirdly in this model because it takes different params
# (y_true, y_predicted_prob) as opposed to (y_true, y_predicted_label),
# which is what cross_val_score assumes
