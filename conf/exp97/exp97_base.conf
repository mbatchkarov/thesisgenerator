name = exp97
debug = False
training_data = sample-data/movie-reviews-tagged
#test_data = ------------------------
output_dir = ./conf/exp97/output
joblib_caching = True
min_test_features = 1
min_train_features = 5

[feature_extraction]
    run = True
    vectorizer = thesisgenerator.plugins.bov.ThesaurusVectorizer
    input = content
    decode_error = replace
    stop_words = english
    min_df = 10
    ngram_range = 1, 1
    ngram_range_decode = 0, 0    # not interested in unigrams at decode time
    unigram_feature_pos_tags = N, J
    analyzer = ngram
    sim_threshold = 0.01
    k = 3
    lemmatize = True
    use_pos = True
    coarse_pos = True
    normalise_entities = False
    use_tfidf = False
    replace_all = False
    record_stats = True
    sim_compressor = thesisgenerator.utils.misc.one
    decode_token_handler = thesisgenerator.plugins.bov_feature_handlers.SignifiedOnlyFeatureHandler
    extract_AN_features = True
    extract_NN_features = True
    extract_VO_features = False
    extract_SVO_features = False
    remove_features_with_NER = True
    random_neighbour_thesaurus = False
[tokenizer]
    lowercase = True
    keep_only_IT = False
    remove_stopwords = True
    remove_short_words = False
    remove_long_words = True    # HDF has a max string length and crashes sometimes. >25 chars is probably noise anyway.
[crossvalidation]
    run = True
    type = skfold
    k = 20
    sample_size = ------------------------
    random_state = 0
[feature_selection]
    run = True    # needed to run filtering based on log odds score
    ensure_vectors_exist = False    # turning this on when "entry_types_to_load = AN, NN" means only AN/NN features will be extracted, which contradicts "unigram_feature_pos_tags=N,J". I've turned it off, which means some of the features will not be in thesaurus and thus will never be seen at train time, thus taking some probability mass away from the other features.
    min_log_odds_score = 1.0    # remove features that are associated with both classes
    k = 99999999999    # do not use chi2 

[classifiers]
    [[sklearn.naive_bayes.MultinomialNB]]
        run = True
        alpha = 0.001
    
    [[sklearn.linear_model.LogisticRegression]]
        run = True
        C = 1e-05
    [[sklearn.naive_bayes.BernoulliNB]]
    [[sklearn.neighbors.KNeighborsClassifier]]
    [[sklearn.svm.SVC]]
    [[sklearn.svm.LinearSVC]]
    [[thesisgenerator.classifiers.MostCommonLabelClassifier]]
    [[thesisgenerator.classifiers.MultinomialNBWithBinaryFeatures]]

[evaluation]
    [[sklearn.metrics.precision_score]]
        run = True
    [[sklearn.metrics.recall_score]]
        run = True
    [[sklearn.metrics.f1_score]]
        run = True
    [[thesisgenerator.metrics.macroavg_prec]]
        run = True
    [[thesisgenerator.metrics.macroavg_rec]]
        run = True
    [[thesisgenerator.metrics.macroavg_f1]]
        run = True
    [[thesisgenerator.metrics.microavg_prec]]
        run = True
    [[thesisgenerator.metrics.microavg_rec]]
        run = True
    [[thesisgenerator.metrics.microavg_f1]]
        run = True
    [[sklearn.metrics.accuracy_score]]
        run = True
[vector_sources]
    reduce_dimensionality = False
    sim_threshold = 0.01
    precomputed = True
    unigram_paths = /mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/exp10-13bAN_NN_gigaw-100_Add/AN_NN_gigaw-100_Add.sims.neighbours.strings,
    include_self = False
    entry_types_to_load = AN, NN
    max_neighbours = 2000
    allow_lexical_overlap = False
    [[thesisgenerator.composers.vectorstore.AdditiveComposer]]
    [[thesisgenerator.composers.vectorstore.MultiplicativeComposer]]
    [[thesisgenerator.composers.vectorstore.BaroniComposer]]
[split_data]
[dimensionality_reduction]
