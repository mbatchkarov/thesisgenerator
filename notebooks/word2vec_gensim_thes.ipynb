{
 "metadata": {
  "name": "",
  "signature": "sha256:ece163bd3f3cb73e07a92a3e487be415c4ffb21c5147cc6e233988e7c189f672"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook has been added to `thesisgenerator` as `get_word2vec_vectors`\n",
      "=============="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim, os, logging, errno\n",
      "from discoutils.thesaurus_loader import Thesaurus\n",
      "from discoutils.tokens import DocumentFeature\n",
      "from collections import defaultdict\n",
      "import numpy as np\n",
      "os.chdir('/mnt/lustre/scratch/inf/mmb28/thesisgenerator')\n",
      "from thesisgenerator.plugins.tokenizers import XmlTokenizer\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# inputs\n",
      "conll_data_dir = '/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/data/gigaword-afe-split-tagged-parsed/gigaword/'\n",
      "pos_only_data_dir = '/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/data/gigaword-afe-split-pos/gigaword/'\n",
      "# outputs\n",
      "unigram_events_file = '/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/word2vec_vectors/thesaurus/word2vec.events.filtered.strings'\n",
      "unigram_thes_file = '/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/word2vec_vectors/word2vec.unigram.thesaurus.txt'\n",
      "\n",
      "pos_map = XmlTokenizer.pos_coarsification_map"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Data formatting\n",
      "=========\n",
      "`word2vec` produces vectors for words, such as `computer`, whereas the rest of my experiments assume there are augmented with a PoS tag, e.g. `computer/N`. To get around that, start with a directory of conll-formatted files such as \n",
      "```\n",
      "1\tAnarchism\tAnarchism\tNNP\tMISC\t5\tnsubj\n",
      "2\tis\tbe\tVBZ\tO\t5\tcop\n",
      "3\ta\ta\tDT\tO\t5\tdet\n",
      "4\tpolitical\tpolitical\tJJ\tO\t5\tamod\n",
      "5\tphilosophy\tphilosophy\tNN\tO\t0\troot\n",
      "```\n",
      "\n",
      "and convert them to pos-augmented format (using coarse tags like Petrov's):\n",
      "\n",
      "```\n",
      "Anarchism/N is/V a/DET ....\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    os.makedirs(pos_only_data_dir)\n",
      "except OSError as exc: # Python >2.5\n",
      "    if exc.errno == errno.EEXIST and os.path.isdir(pos_only_data_dir):\n",
      "        pass\n",
      "    else: raise\n",
      "\n",
      "for filename in os.listdir(conll_data_dir):\n",
      "    outfile_name = os.path.join(pos_only_data_dir, '%s_nopos'%filename)\n",
      "    logging.info('Reformatting %s to %s', filename, outfile_name)\n",
      "    with open(os.path.join(conll_data_dir, filename)) as infile, open(outfile_name, 'w') as outfile:\n",
      "        for line in infile:\n",
      "            if not line.strip(): # conll empty line = sentence boundary\n",
      "                outfile.write('.\\n')\n",
      "                continue\n",
      "            idx, word, lemma, pos, ner, dep, _ = line.strip().split('\\t')\n",
      "            outfile.write('%s '%lemma.lower())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train a word2vec model\n",
      "class MySentences(object):\n",
      "    def __init__(self, dirname):\n",
      "        self.dirname = dirname\n",
      "\n",
      "    def __iter__(self):\n",
      "        for fname in os.listdir(self.dirname):\n",
      "            print(self.dirname)\n",
      "            for line in open(os.path.join(self.dirname, fname)):\n",
      "                yield line.split()\n",
      "\n",
      "\n",
      "logging.info('Training word2vec')\n",
      "sentences = MySentences(pos_only_data_dir)  # a memory-friendly iterator\n",
      "model = gensim.models.Word2Vec(sentences, workers=4, min_count=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/data/gigaword-afe-split-pos/gigaword/\n",
        "/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/data/gigaword-afe-split-pos/gigaword/"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/data/gigaword-afe-split-pos/gigaword/"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/data/gigaword-afe-split-pos/gigaword/"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sanity check\n",
      "print model.most_similar('computer/N', topn=20)\n",
      "print model['computer/N'].shape\n",
      "print model.similarity('computer/N', 'software/N')\n",
      "from scipy.spatial.distance import cosine\n",
      "print 1-cosine(model['computer/N'], model['software/N'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 19:14:38,326 : INFO : precomputing L2-norms of word weight vectors\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('software/N', 0.7612380981445312), ('card/N', 0.6549723148345947), ('ibm/N', 0.6414561867713928), ('design/N', 0.6359105110168457), ('model/N', 0.6070767641067505), ('hardware/N', 0.6045330762863159), ('architecture/N', 0.5947156548500061), ('sport/N', 0.594567060470581), ('new/J', 0.5853791236877441), ('system/N', 0.5803729295730591), ('machine/N', 0.5755997896194458), ('apl/N', 0.5753541588783264), ('amiga/N', 0.5686793327331543), ('car/N', 0.5571242570877075), ('disk/N', 0.5566810369491577), ('program/N', 0.5547628402709961), ('video/N', 0.5531482100486755), ('project/N', 0.5450963973999023), ('product/N', 0.5232951641082764), ('apple/N', 0.5095471739768982)]\n",
        "(100,)\n",
        "0.761238145462\n",
        "0.761238217354\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get word2vec vectors for each word, write to TSV\n",
      "vectors = dict()\n",
      "dimension_names = ['word2vec_feat%02d'%i for i in range(100)] # word2vec produces 100-dim vectors\n",
      "for word in model.vocab.keys():\n",
      "    vectors[word] = zip(dimension_names, model[word])\n",
      "th1 = Thesaurus(vectors)\n",
      "th1.to_tsv(unigram_events_file, preserve_order=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 19:14:38,404 : WARNING : row_transform and entry_filter options are ignored in order to use preserve_order\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "'/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/word2vec_vectors/thesaurus/word2vec.events.filtered.strings'"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build a thesaurus out of the nearest neighbours of each unigram and save it to TSV\n",
      "# this is a little incompatible with the rest of my thesauri as it uses the first PoS-augmented form for each word2vec word is used\n",
      "# nevertheless, it's useful to compare these neighbours to Byblo's neighbours as a sanity check\n",
      "mythes = dict()\n",
      "for word in model.vocab.keys():\n",
      "    if word == 'computer/N':\n",
      "        print model.most_similar(word, topn=10)\n",
      "    mythes[word] = model.most_similar(word, topn=10)\n",
      "print len(mythes), np.mean([len(foo) for foo in mythes.values()])\n",
      "Thesaurus(mythes).to_tsv(unigram_thes_file, preserve_order=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('software/N', 0.7612380981445312), ('card/N', 0.6549723148345947), ('ibm/N', 0.6414561867713928), ('design/N', 0.6359105110168457), ('model/N', 0.6070767641067505), ('hardware/N', 0.6045330762863159), ('architecture/N', 0.5947156548500061), ('sport/N', 0.594567060470581), ('new/J', 0.5853791236877441), ('system/N', 0.5803729295730591)]\n",
        "2077"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 19:14:39,022 : WARNING : row_transform and entry_filter options are ignored in order to use preserve_order\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10.0\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "'/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/word2vec_vectors/word2vec.unigram.thesaurus.txt'"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print model['computer/N'][:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.14103925 -0.05403119 -0.09513755  0.03750326  0.26305121 -0.18209456\n",
        " -0.04893927 -0.12527311 -0.00802974 -0.2132474 ]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = Thesaurus.from_tsv([unigram_events_file])\n",
      "comp_features = [foo[0] for foo in t['computer/N']]\n",
      "comp_vector = [foo[1] for foo in t['computer/N']]\n",
      "# print comp_vector[:10]\n",
      "print comp_features[:10]\n",
      "# print t['computer/N']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 19:14:39,045 : INFO : Loading thesaurus /mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/word2vec_vectors/thesaurus/word2vec.events.filtered.strings from disk\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 19:14:39,209 : ERROR : Cannot create token out of string \\//PUNCT\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 19:14:39,349 : ERROR : Cannot create token out of string .\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['word2vec_feat00', 'word2vec_feat01', 'word2vec_feat02', 'word2vec_feat03', 'word2vec_feat04', 'word2vec_feat05', 'word2vec_feat06', 'word2vec_feat07', 'word2vec_feat08', 'word2vec_feat09']\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    }
   ],
   "metadata": {}
  }
 ]
}