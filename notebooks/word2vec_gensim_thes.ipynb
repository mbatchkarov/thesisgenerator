{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook has been added to `thesisgenerator` as `get_word2vec_vectors`\n",
      "=============="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim, os, logging, errno\n",
      "from discoutils.thesaurus_loader import Thesaurus\n",
      "from discoutils.tokens import DocumentFeature\n",
      "from collections import defaultdict\n",
      "import numpy as np\n",
      "os.chdir('/mnt/lustre/scratch/inf/mmb28/thesisgenerator')\n",
      "from thesisgenerator.plugins.tokenizers import XmlTokenizer\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# inputs\n",
      "conll_data_dir = '/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/data/gigaword-afe-split-tagged-parsed/gigaword/'\n",
      "pos_only_data_dir = '/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/data/gigaword-afe-split-pos/gigaword/'\n",
      "# outputs\n",
      "unigram_events_file = '/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/word2vec_vectors/thesaurus/word2vec.events.filtered.strings'\n",
      "unigram_thes_file = '/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/word2vec_vectors/word2vec.unigram.thesaurus.txt'\n",
      "\n",
      "pos_map = XmlTokenizer.pos_coarsification_map"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Data formatting\n",
      "=========\n",
      "`word2vec` produces vectors for words, such as `computer`, whereas the rest of my experiments assume there are augmented with a PoS tag, e.g. `computer/N`. To get around that, start with a directory of conll-formatted files such as \n",
      "```\n",
      "1\tAnarchism\tAnarchism\tNNP\tMISC\t5\tnsubj\n",
      "2\tis\tbe\tVBZ\tO\t5\tcop\n",
      "3\ta\ta\tDT\tO\t5\tdet\n",
      "4\tpolitical\tpolitical\tJJ\tO\t5\tamod\n",
      "5\tphilosophy\tphilosophy\tNN\tO\t0\troot\n",
      "```\n",
      "\n",
      "and convert them to pos-augmented format (using coarse tags like Petrov's):\n",
      "\n",
      "```\n",
      "Anarchism/N is/V a/DET ....\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "try:\n",
      "    os.makedirs(pos_only_data_dir)\n",
      "except OSError as exc: # Python >2.5\n",
      "    if exc.errno == errno.EEXIST and os.path.isdir(pos_only_data_dir):\n",
      "        pass\n",
      "    else: raise\n",
      "\n",
      "for filename in os.listdir(conll_data_dir):\n",
      "    outfile_name = os.path.join(pos_only_data_dir, filename)\n",
      "    logging.info('Reformatting %s to %s', filename, outfile_name)\n",
      "    with open(os.path.join(conll_data_dir, filename)) as infile, open(outfile_name, 'w') as outfile:\n",
      "        for line in infile:\n",
      "            if not line.strip(): # conll empty line = sentence boundary\n",
      "                outfile.write('.\\n')\n",
      "                continue\n",
      "            idx, word, lemma, pos, ner, dep, _ = line.strip().split('\\t')\n",
      "            outfile.write('%s/%s '%(lemma.lower(), pos_map[pos]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:02,515 : INFO : Reformatting conll-sample to /mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/data/gigaword-afe-split-pos/gigaword/conll-sample\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train a word2vec model\n",
      "class MySentences(object):\n",
      "    def __init__(self, dirname):\n",
      "        self.dirname = dirname\n",
      "\n",
      "    def __iter__(self):\n",
      "        for fname in os.listdir(self.dirname):\n",
      "            for line in open(os.path.join(self.dirname, fname)):\n",
      "                yield line.split()\n",
      "\n",
      "\n",
      "logging.info('Training word2vec')\n",
      "sentences = MySentences(pos_only_data_dir)  # a memory-friendly iterator\n",
      "model = gensim.models.Word2Vec(sentences, workers=4, min_count=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:05,738 : INFO : Training word2vec\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:05,739 : INFO : collecting all words and their counts\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:05,739 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:05,852 : INFO : PROGRESS: at sentence #10000, processed 274160 words and 21143 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:05,962 : INFO : PROGRESS: at sentence #20000, processed 553070 words and 33965 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:06,096 : INFO : PROGRESS: at sentence #30000, processed 839479 words and 44489 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:06,205 : INFO : PROGRESS: at sentence #40000, processed 1123059 words and 54047 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:06,310 : INFO : PROGRESS: at sentence #50000, processed 1398096 words and 62809 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:06,415 : INFO : PROGRESS: at sentence #60000, processed 1679920 words and 70327 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:06,523 : INFO : PROGRESS: at sentence #70000, processed 1964108 words and 78581 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:06,553 : INFO : collected 81273 word types from a corpus of 2034765 words and 72514 sentences\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:06,579 : INFO : total 2077 word types after removing those with count<100\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:06,579 : INFO : constructing a huffman tree from 2077 words\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:06,629 : INFO : built huffman tree with maximum node depth 14\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:06,630 : INFO : resetting layer weights\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:06,688 : INFO : training model with 4 workers on 2077 vocabulary and 100 features, using 'skipgram'=1 'hierarchical softmax'=1 and 'negative sampling'=0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:07,689 : INFO : PROGRESS: at 37.26% words, alpha 0.01584, 600698 words/s\n",
        "2014-07-05 15:24:08,689 : INFO : PROGRESS: at 69.68% words, alpha 0.00768, 561800 words/s\n",
        "2014-07-05 15:24:09,471 : INFO : reached the end of input; waiting to finish 8 outstanding jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:09,500 : INFO : training on 1612884 words took 2.8s, 573688 words/s\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# sanity check\n",
      "print model.most_similar('computer/N', topn=20)\n",
      "print model['computer/N'].shape\n",
      "print model.similarity('computer/N', 'software/N')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('software/N', 0.7645787596702576), ('amiga/N', 0.6695442199707031), ('system/N', 0.6637003421783447), ('ibm/N', 0.6467227935791016), ('design/N', 0.6452583074569702), ('apl/N', 0.611747145652771), ('machine/N', 0.6108277440071106), ('hardware/N', 0.6095170974731445), ('card/N', 0.5834347605705261), ('disk/N', 0.575678825378418), ('sport/N', 0.5743871927261353), ('model/N', 0.5705682039260864), ('project/N', 0.566510796546936), ('new/J', 0.5596661567687988), ('popular/J', 0.5450026392936707), ('program/N', 0.5268867015838623), ('character/N', 0.5239413976669312), ('video/N', 0.5195738077163696), ('music/N', 0.5108837485313416), ('apple/N', 0.5006910562515259)]\n",
        "(100,)\n",
        "0.76457869291\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# get word2vec vectors for each word, write to TSV\n",
      "vectors = dict()\n",
      "dimension_names = ['word2vec_feat%02d'%i for i in range(100)] # word2vec produces 100-dim vectors\n",
      "for word in model.vocab.keys():\n",
      "    vectors[word] = zip(dimension_names, model[word])\n",
      "th1 = Thesaurus(vectors)\n",
      "th1.to_tsv(unigram_events_file, preserve_order=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:27:16,878 : WARNING : row_transform and entry_filter options are ignored in order to use preserve_order\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "'/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/word2vec_vectors/thesaurus/word2vec.events.filtered.strings'"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build a thesaurus out of the nearest neighbours of each unigram and save it to TSV\n",
      "# this is a little incompatible with the rest of my thesauri as it uses the first PoS-augmented form for each word2vec word is used\n",
      "# nevertheless, it's useful to compare these neighbours to Byblo's neighbours as a sanity check\n",
      "mythes = dict()\n",
      "for word in model.vocab.keys():\n",
      "    if word == 'computer/N':\n",
      "        print model.most_similar(word, topn=10)\n",
      "    mythes[word] = model.most_similar(word, topn=10)\n",
      "print len(mythes), np.mean([len(foo) for foo in mythes.values()])\n",
      "Thesaurus(mythes).to_tsv(unigram_thes_file, preserve_order=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[('software/N', 0.7645787596702576), ('amiga/N', 0.6695442199707031), ('system/N', 0.6637003421783447), ('ibm/N', 0.6467227935791016), ('design/N', 0.6452583074569702), ('apl/N', 0.611747145652771), ('machine/N', 0.6108277440071106), ('hardware/N', 0.6095170974731445), ('card/N', 0.5834347605705261), ('disk/N', 0.575678825378418)]\n",
        "2077"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-05 15:24:12,578 : WARNING : row_transform and entry_filter options are ignored in order to use preserve_order\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 10.0\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "'/mnt/lustre/scratch/inf/mmb28/FeatureExtrationToolkit/word2vec_vectors/word2vec.unigram.thesaurus.txt'"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print model['computer/N'][:60]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.0877879  -0.06064482 -0.14523113  0.25981808  0.03729434 -0.01704549\n",
        " -0.05127664 -0.09765378 -0.08352299 -0.06057465 -0.09149636  0.01606293\n",
        "  0.05523007 -0.00715084 -0.00982234 -0.10349131  0.25728732 -0.0406292\n",
        "  0.14864907 -0.01903271 -0.00730042 -0.20030618  0.24011821 -0.03814232\n",
        "  0.05784744  0.0099752  -0.06263568 -0.03365819  0.09252743  0.00153417\n",
        " -0.09043842  0.3363201   0.04629176  0.19879849  0.13686183 -0.20152816\n",
        " -0.32010788  0.12466857 -0.08133816  0.35740468  0.23994592  0.09430224\n",
        " -0.23537566  0.04972184 -0.22164634  0.03856898 -0.18554164 -0.08539917\n",
        " -0.29693076  0.14835119 -0.0610607  -0.10810128 -0.07025296 -0.3372654\n",
        "  0.19410887 -0.22282848  0.10117316 -0.08131307  0.11031806 -0.28848499]\n"
       ]
      }
     ],
     "prompt_number": 11
    }
   ],
   "metadata": {}
  }
 ]
}