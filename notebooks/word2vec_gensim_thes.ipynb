{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim, os, logging\n",
      "from discoutils.thesaurus_loader import Thesaurus\n",
      "from discoutils.tokens import DocumentFeature\n",
      "from collections import defaultdict\n",
      "import numpy as np\n",
      "os.chdir('/Volumes/LocalDataHD/mmb28/NetBeansProjects/thesisgenerator/')\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class MySentences(object):\n",
      "    def __init__(self, dirname):\n",
      "        self.dirname = dirname\n",
      "\n",
      "    def __iter__(self):\n",
      "        for fname in os.listdir(self.dirname):\n",
      "            for line in open(os.path.join(self.dirname, fname)):\n",
      "                yield line.split()\n",
      "\n",
      "\n",
      "sentences = MySentences('sample-data/wiki-unlabelled-sample')  # a memory-friendly iterator\n",
      "model = gensim.models.Word2Vec(sentences, workers=4, min_count=100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:07,050 : INFO : collecting all words and their counts\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:07,051 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:07,530 : INFO : PROGRESS: at sentence #10000, processed 770277 words and 94845 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:08,001 : INFO : PROGRESS: at sentence #20000, processed 1541487 words and 155439 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:08,447 : INFO : PROGRESS: at sentence #30000, processed 2275818 words and 207625 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:08,834 : INFO : PROGRESS: at sentence #40000, processed 3034652 words and 252421 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:09,295 : INFO : PROGRESS: at sentence #50000, processed 3837269 words and 294954 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:09,734 : INFO : PROGRESS: at sentence #60000, processed 4563649 words and 329213 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:10,114 : INFO : PROGRESS: at sentence #70000, processed 5286715 words and 363523 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:10,496 : INFO : PROGRESS: at sentence #80000, processed 6067598 words and 397927 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:10,959 : INFO : PROGRESS: at sentence #90000, processed 6817519 words and 429704 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:11,338 : INFO : PROGRESS: at sentence #100000, processed 7573610 words and 462744 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:11,708 : INFO : PROGRESS: at sentence #110000, processed 8322785 words and 491860 word types\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:11,725 : INFO : collected 492861 word types from a corpus of 8354018 words and 110339 sentences\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:11,883 : INFO : total 7807 word types after removing those with count<100\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:11,884 : INFO : constructing a huffman tree from 7807 words\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:12,149 : INFO : built huffman tree with maximum node depth 16\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:12,151 : INFO : resetting layer weights\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:12,399 : INFO : training model with 4 workers on 7807 vocabulary and 100 features, using 'skipgram'=1 'hierarchical softmax'=1 and 'negative sampling'=0\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:13,409 : INFO : PROGRESS: at 8.51% words, alpha 0.02294, 542403 words/s\n",
        "2014-07-04 15:26:14,430 : INFO : PROGRESS: at 17.58% words, alpha 0.02068, 556697 words/s\n",
        "2014-07-04 15:26:15,439 : INFO : PROGRESS: at 26.55% words, alpha 0.01845, 561847 words/s\n",
        "2014-07-04 15:26:16,446 : INFO : PROGRESS: at 35.67% words, alpha 0.01617, 566912 words/s\n",
        "2014-07-04 15:26:17,446 : INFO : PROGRESS: at 44.63% words, alpha 0.01395, 568686 words/s\n",
        "2014-07-04 15:26:18,447 : INFO : PROGRESS: at 53.54% words, alpha 0.01174, 569295 words/s\n",
        "2014-07-04 15:26:19,449 : INFO : PROGRESS: at 62.53% words, alpha 0.00944, 570417 words/s\n",
        "2014-07-04 15:26:20,460 : INFO : PROGRESS: at 71.45% words, alpha 0.00720, 570092 words/s\n",
        "2014-07-04 15:26:21,470 : INFO : PROGRESS: at 80.41% words, alpha 0.00499, 570145 words/s\n",
        "2014-07-04 15:26:22,476 : INFO : PROGRESS: at 89.23% words, alpha 0.00280, 569432 words/s\n",
        "2014-07-04 15:26:23,481 : INFO : PROGRESS: at 98.03% words, alpha 0.00050, 568890 words/s\n",
        "2014-07-04 15:26:23,579 : INFO : reached the end of input; waiting to finish 8 outstanding jobs\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:26:23,671 : INFO : training on 6430522 words took 11.3s, 570516 words/s\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.most_similar('computer')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[('software', 0.7395773530006409),\n",
        " ('database', 0.7193808555603027),\n",
        " ('digital', 0.6976978778839111),\n",
        " ('computers,', 0.695641040802002),\n",
        " ('Windows', 0.692505955696106),\n",
        " ('computers', 0.6849778890609741),\n",
        " ('software.', 0.6815366148948669),\n",
        " ('computing', 0.6811175346374512),\n",
        " ('3D', 0.6809398531913757),\n",
        " ('hardware', 0.6781383752822876)]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open('notebooks/MR_R2_feat_freq_in_gigaw.txt') as infile:\n",
      "    feat_freq = defaultdict(int)\n",
      "    all_unigrams = set()\n",
      "    for line in infile:\n",
      "        freq, feature = line.strip().split()\n",
      "        feat_freq[feature] += int(freq)\n",
      "        feature = DocumentFeature.from_string(feature)\n",
      "        if feature.type == '1-GRAM':\n",
      "            all_unigrams.add(feature)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:39:39,184 : ERROR : Cannot create token out of string big/J_:-lrb-_nn/N\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:39:39,194 : ERROR : Cannot create token out of string big/N_:-lrb-_nn/N\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:39:39,301 : ERROR : Cannot create token out of string boat/N_:-lrb-_nn/N\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:39:40,176 : ERROR : Cannot create token out of string class/N_:-lrb-_nn/N\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:39:41,070 : ERROR : Cannot create token out of string day/N_:-lrb-_nn/N\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:39:41,490 : ERROR : Cannot create token out of string d/N__/N\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:39:42,024 : ERROR : Cannot create token out of string equal/J_:-lrb-_nn/N\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:39:45,482 : ERROR : Cannot create token out of string l/N_:-lrb-_nn/N\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:39:46,948 : ERROR : Cannot create token out of string n/N_;-rrb-_nn/N\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:39:47,950 : ERROR : Cannot create token out of string point/N_:-rrb-_nn/N\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:39:51,255 : ERROR : Cannot create token out of string total/J_:-rrb-_nn/N\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 15:39:51,411 : ERROR : Cannot create token out of string trophy/N_:-lrb-_nn/N\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word2vec_entries = set(model.vocab.keys())\n",
      "feat_to_word = dict()\n",
      "word_to_feat = dict()\n",
      "for feat in all_unigrams:\n",
      "    word = feat.tokens[0].text\n",
      "    if word in word2vec_entries:\n",
      "        feat_to_word[feat]=word\n",
      "        word_to_feat[word]=feat\n",
      "print len(word2vec_entries), len(all_unigrams), len(word_to_feat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "7807 27683 2946\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mythes = dict()\n",
      "for feature, word in feat_to_word.items():\n",
      "    pos = feature.tokens[0].pos\n",
      "    neighbours = []\n",
      "    for rank, (neigh, sim) in enumerate(model.most_similar(word, topn=10)):\n",
      "        if neigh in word_to_feat:\n",
      "            neighbours.append((word_to_feat[neigh].tokens_as_str(), sim))\n",
      "    if neighbours:\n",
      "        mythes[feature.tokens_as_str()] = neighbours\n",
      "print len(mythes), np.mean([len(foo) for foo in mythes.values()])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3298 6.38659793814\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Thesaurus(mythes).to_tsv('word2vec.thes.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 16:08:45,762 : INFO : Writing events to word2vec.thes.txt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "2014-07-04 16:08:45,763 : INFO : Processed 0 vectors\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 62,
       "text": [
        "'word2vec.thes.txt'"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}